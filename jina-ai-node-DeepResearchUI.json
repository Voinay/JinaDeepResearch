[{"id":"deepresearch","user_id":"4d2fcf05-9645-47d0-9d09-dcd5368d0556","name":"DeepResearch","type":"pipe","content":"\"\"\"\ntitle: Deep Research\nauthor: Voinay\ndescription: 在OpwenWebUI中支持jina-ai/node-DeepResearch的思维链和回复模型分离 - 仅支持0.5.6及以上版本\ngithub: \nversion: 1.0\nlicence: MIT\n\"\"\"\n\nimport json\nimport httpx\nimport re\nfrom typing import AsyncGenerator, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nimport asyncio\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        DEEPRESEARCH: str = Field(\n            default=\"http://localhost:3000/v1\",\n            description=\"DeepResearch本地的基础请求地址\",\n        )\n        DEEPRESEARCH_API_MODEL: str = Field(\n            default=\"gemini-2.0-flash\",\n            description=\"API请求的模型名称，默认为 gemini-2.0-flash \",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.data_prefix = \"data:\"\n        self.emitter = None\n        self.current_response_id = None\n        self.client = httpx.AsyncClient(http2=True)  # 创建一个持久化的客户端\n\n    def pipes(self):\n        return [\n            {\n                \"id\": self.valves.DEEPRESEARCH_API_MODEL,\n                \"name\": self.valves.DEEPRESEARCH_API_MODEL,\n            }\n        ]\n\n    async def pipe(\n        self, body: dict, __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"主处理管道（已移除缓冲）\"\"\"\n        thinking_state = {\"thinking\": -1}  # 使用字典来存储thinking状态\n        self.emitter = __event_emitter__\n\n        # 准备请求参数\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        try:\n            # 模型ID提取\n            model_id = body[\"model\"].split(\".\", 1)[-1]\n            payload = {**body, \"model\": model_id}\n\n            # 提取当前消息并重置消息列表\n            current_message = payload[\"messages\"][-1]\n            payload[\"messages\"] = [current_message]\n\n            # 发起API请求\n            async with self.client.stream(\n                \"POST\",\n                f\"{self.valves.DEEPRESEARCH}/chat/completions\",\n                json=payload,\n                headers=headers,\n                timeout=300,\n            ) as response:\n                # 错误处理\n                if response.status_code != 200:\n                    error = await response.aread()\n                    yield self._format_error(response.status_code, error)\n                    return\n\n                # 流式处理响应\n                async for line in response.aiter_lines():\n                    if not line.startswith(self.data_prefix):\n                        continue\n\n                    # 截取 JSON 字符串\n                    json_str = line[len(self.data_prefix) :]\n\n                    try:\n                        data = json.loads(json_str)\n                    except json.JSONDecodeError as e:\n                        # 格式化错误信息，这里传入错误类型和详细原因（包括出错内容和异常信息）\n                        error_detail = f\"解析失败 - 内容：{json_str}，原因：{e}\"\n                        yield self._format_error(\"JSONDecodeError\", error_detail)\n                        return\n\n                    choice = data.get(\"choices\", [{}])[0]\n                    delta = choice.get(\"delta\", {})\n                    response_id = data.get(\"id\")\n\n                    # 检查是否是新的响应\n                    if response_id != self.current_response_id:\n                        self.current_response_id = response_id\n                        thinking_state[\"thinking\"] = -1  # 重置思考状态\n\n                    # 结束条件判断\n                    finish_reason = choice.get(\"finish_reason\")\n\n                    # 状态机处理\n                    state_output = await self._update_thinking_state(\n                        delta, thinking_state\n                    )\n                    if state_output:\n                        yield state_output\n                        if state_output == \"<think>\":\n                            yield \"\\n\"\n\n                    # 内容处理\n                    content = self._process_content(delta)\n                    if content:\n                        yield content\n\n                    # 在发送内容后处理结束条件\n                    if finish_reason == \"stop\":\n                        return\n\n        except Exception as e:\n            yield self._format_exception(e)\n\n    async def _update_thinking_state(self, delta: dict, thinking_state: dict) -> str:\n        \"\"\"更新思考状态机（简化版）\"\"\"\n        state_output = \"\"\n\n        # 状态转换：未开始 -> 思考中\n        if thinking_state[\"thinking\"] == -1 and delta.get(\"content\") == \"<think>\":\n            thinking_state[\"thinking\"] = 0\n            state_output = \"<think>\"\n\n        # 状态转换：思考中 -> 已回答\n        elif thinking_state[\"thinking\"] == 0 and delta.get(\"content\") == \"</think>\\n\\n\":\n            thinking_state[\"thinking\"] = 1\n            state_output = \"\\n\\n\"\n\n        return state_output\n\n    def _process_content(self, delta: dict) -> str:\n        \"\"\"直接返回处理后的内容\"\"\"\n        return delta.get(\"content\", \"\")\n\n    def _format_error(self, status_code: int, error: bytes) -> str:\n        # 如果 error 已经是字符串，则无需 decode\n        if isinstance(error, str):\n            error_str = error\n        else:\n            error_str = error.decode(errors=\"ignore\")\n\n        try:\n            err_msg = json.loads(error_str).get(\"message\", error_str)[:200]\n        except Exception as e:\n            err_msg = error_str[:200]\n        return json.dumps(\n            {\"error\": f\"HTTP {status_code}: {err_msg}\"}, ensure_ascii=False\n        )\n\n    def _format_exception(self, e: Exception) -> str:\n        \"\"\"异常格式化保持不变\"\"\"\n        err_type = type(e).__name__\n        return json.dumps({\"error\": f\"{err_type}: {str(e)}\"}, ensure_ascii=False)\n","meta":{"description":"在OpwenWebUI中显示本地DeepResearch模型的思维链","manifest":{"title":"Deep Research","author":"Voinay","description":"在OpwenWebUI中支持jina-ai/node-DeepResearch的思维链和回复模型分离 - 仅支持0.5.6及以上版本","github":"","version":"1.0","licence":"MIT"}},"is_active":true,"is_global":false,"updated_at":1739195114,"created_at":1739116366}]